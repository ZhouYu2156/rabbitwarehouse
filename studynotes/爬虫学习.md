# 爬虫笔记

## 1、内置urllib库

**（1）urllib.request.urlopen()方法**

```python
import urllib.request

# (1)定义url 需要访问的网址
url = 'http://www.baidu.com/'
# (2)模拟浏览器向服务器发送请求 response响应
response = urllib.request.urlopen(url)
# (3)获取响应中的页面的源码，content 内容的意思
# read方法 返回的是字节形式的二进制数据
# 我们需要将二进制的数据转换为字符串
# 二进制 => 字符串  称为解码  decode('编码的格式')
content = response.read().decode('utf-8')
print(content)
```

**（2）一个类型和六个方法**

- 响应对象response是HTTPResponse类型
- response.read()方法
- response.read(number)方法表示读取number个字节
- response.readline()方法读取一行
- response.readlines()方法读取多行
- response.getcode()方法获取状态码
- response.geturl()方法获取访问地址
- response.getheaders()方法返回状态信息

**（3）重要方法：**

```python
# 参数：url, filename（带后缀的文件名）=>可以直接访问网址并进行下载文件，包括文本、图片、视频
urllib.request.urlretrieve(url, filename)
```



**（4）url的组成：**

```python
# http/https    www.baidu.com   80/443
# 协议            主机            端口号   路径  参数  锚点
# http  80
# https 443
# mysql   3306
# oracle  1521
# redis   6379
# mongodb 27017
```

**（5）urllib.request.urlopen()方法遇到反爬时需要请求对象的定制：**

```python
request = urllib.request.Request(url=url, headers=headers)
# 传入请求对象
urllib.request.urlopen(request)
```

（6）urllib的GET请求中的参数需要ulilib.parse.quote()方法将中文字符转换为ASCII编码

```python
import urllib.parse
base_url = 'http://www.baidu.com/s?'
data = {
    'wd': '周杰伦',
    'sex': '男'
}
content = urllib.parse.quote(data)	# => 周星驰 = %E5%91%A8%E6%98%9F%E9%A9%B0
print(content)
```

（7）urllib的POST请求中的参数需要urllib.parse.urlencode(data).encode()

```python
import urllib.parse

data = {
    'wd': '周杰伦',
    'sex': '男'
}
# urllib的POST请求
# 需要进行urlencode编码(data中有中文的话 => 需要调用encode()方法进行字节类型的encode编码)
data = urllib.parse.urlencode(data).encode('utf-8')
print(data)

# 之后获取的响应内容还需要进行解码操作	request是请求对象定制之后的request
response = urllib.request.urlopen(request)
# 还需要进行encode对应的解码操作
content = response.read().decode('utf-8')
# 如果获取到的是json数据，还需要进行json数据的转换
import json
obj = json.loads(content)
print(obj)
```

（8）handler处理器   =>   之后会用来处理器来加工代理

```python
import urllib.request
# 创建request请求对象
request = urllib.request.Request(url = url, headers = headers)
# (1)获取handler对象
handler = urllib.request.HTTPHandler()
# (2)获取opener对象
opener = urllib.request.build_opener(handler)
# (3)调用open方法
response = opener.open(request)
# 接下来可以打印一下响应内容了
content = response.read().decode('utf-8')
print(content)
```



（9）ProxyHandler => IP代理

```python
url = 'xxx'
headers = {'user-agent': 'xxxx'}
# 请求对象定制
request = urllib.request.Request(url = url, headers = headers)
# 代理
proxies = {
    'http': '127.0.0.1'		# ip地址
}
# 处理器对象
handler = urllib.request.ProxyHandler(proxies = proxies)
# 创建opener对象
opener = urllib.request.build_opener(handler)
# 获取响应对象
response = opener.open(request)
# 打印看内容
content = response.read().decode('utf-8')
print(content)
```

## 2、xpath语法

#### 1.路经查询

- //：查找所有子孙节点，忽略层级关系
- /：查找直接子节点

#### 2.谓词查询

- //div[@id]
- //div[@id="maincontent"]

#### 3.属性查询

- //@class

#### 4.模糊查询

- //div[contains(@id,"he")]
- //div[starts-with(@id,"he")]

#### 5.内容查询

- //div/h1/text()

#### 6.逻辑运算

- //div[@id="head" and @class="s_down"]
- //title | //price   => '//ul/li[@id="l1"]/text() | //ul/li[@id="c1"]/text()'

**下载解析模块lxml**

```cmd
pip install lxml
```

```python
# 导入使用
import etree from lxml
# 有两种使用情况
# 第一种解析本地html文件 => 用etree.parse()
html = etree.parse("xxx.html")
# 将文件利用parse转换为树形模型之后，就可以使用xpath语法对其进行解析提取内容了
# 示例:
content = html.xpath("//div/h1/text()")
print(content)

# 第二种是解析服务器响应文件   => 用etree.HTML()
content = response.read().decode('utf-8')
html = etree.HTML(content)
# 之后就可以使用xpath进行解析了
# 示例:
elementId = html.xpath("//div/h1/@id")
print(elementId)
```

## 3、jsonpath语法

**jsonpath的安装及使用方式**

```cmd
pip install jsonpath
```

```python
import json
import jsonpath
# jsonpath的使用
# 第一步 => 用json.load()加载json文件，用open()方法打开文件给json.load()加载
obj = json.load(open('json文件', 'r', encoding='utf-8'))
# 第二部 => jsonpath语法解析数据
ret = jsonpath.jsonpath(obj, 'jsonpath语法')
```

**jsonpath语法学习推荐博客地址：**https://blog.csdn.net/luxideyao/article/details/77802389

## 4、BeautifulSoup语法

## 5、selenium浏览器驱动器

```python
# 下载selenium	=> pip install selenium
# 导入使用
import webdriver from selenium
# 创建浏览器驱动对象
path = 'chromedriver.exe'	# => 驱动器路径
browser = webdriver.Chrome(path)
# 基本的使用 => 打开网页
browser.get('https://www.baidu.com/')

# 获取网页元素方法
# 1、通过id
#browser.find_element_by_id()
# 2、通过xpath
# browser.find_elements_by_xpath()	=>括号里写xpath语法
# 3、通过select
# browser.find_elements_by_css_select()	=>写css选择器的语法
```

































